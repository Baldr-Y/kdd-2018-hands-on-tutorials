{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GitHub issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://storage.googleapis.com/kubeflow-examples/github-issue-summarization-data/github-issues.zip\n",
    "!unzip github-issues.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1: Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from numpy import array\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding\n",
    "from ktext.preprocess import processor\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('train.docstring', 'r') as f:\n",
    "#     train_doc = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "issues = pd.read_csv('github_issues.csv')\n",
    "docs = list(issues.body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc = processor(hueristic_pct_padding=.7,\n",
    "                 keep_n=20000)\n",
    "vecs = proc.fit_transform(docs[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = max(proc.id2token.keys()) + 1\n",
    "max_length = proc.padding_maxlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "sequences = []\n",
    "for arr in tqdm(vecs):\n",
    "    non_zero = (arr != 0).argmax()\n",
    "    for i in range(non_zero, len(arr)):\n",
    "        sequence = arr[:i+1]\n",
    "        sequences.append(sequence)\n",
    "sequences = pad_sequences(sequences, maxlen=max_length, padding='pre')\n",
    "sequences = array(sequences)\n",
    "X, y = sequences[:,:-1], sequences[:,-1]\n",
    "y = to_categorical(y, num_classes=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, LSTM, Embedding, Lambda\n",
    "\n",
    "i = Input(shape=(max_length-1,))\n",
    "o = Embedding(vocab_size, 128, input_length=max_length-1)(i)\n",
    "o = LSTM(50, return_sequences=True)(o)\n",
    "last_timestep = Lambda(lambda x: x[:, -1, :])(o)\n",
    "last_timestep = Dense(vocab_size, activation='softmax')(last_timestep)\n",
    "model = Model(i, last_timestep)\n",
    "print(model.summary())\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X, y, epochs=20, batch_size=2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_seq(model, proc, max_length, seed_text, n_words):\n",
    "    in_text = seed_text\n",
    "    # generate a fixed number of words\n",
    "    for _ in range(n_words):\n",
    "        # encode the text as integer\n",
    "        encoded = proc.transform([in_text])[:,1:]\n",
    "        # pre-pad sequences to a fixed length\n",
    "        yhat = np.argmax(model.predict(encoded, verbose=0), axis=1)[0]\n",
    "        # map predicted word index to word\n",
    "        out_word = ''\n",
    "        if yhat == 1:\n",
    "            out_word = '_unk_'\n",
    "        else:\n",
    "            out_word = proc.id2token[yhat]\n",
    "        in_text += ' ' + out_word\n",
    "    return in_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_seq(model, proc, max_length, 'there', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = 'def machine learning'\n",
    "encoded = proc.transform([seq])[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_model = Model(inputs=model.inputs, outputs=model.layers[-3].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_model.predict(encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2: Sequence to Sequence Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_docs = list(issues.issue_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ktext.preprocess import processor\n",
    "func_proc = processor(hueristic_pct_padding=.7,\n",
    "                      keep_n=20000)\n",
    "func_vecs = func_proc.fit_transform(docs[:1000])\n",
    "\n",
    "doc_proc = processor(append_indicators=True,\n",
    "                     hueristic_pct_padding=.7,\n",
    "                     keep_n=14000, padding ='post')\n",
    "doc_vecs = doc_proc.fit_transform(target_docs[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_data = func_vecs\n",
    "encoder_seq_len = encoder_input_data.shape[1]\n",
    "\n",
    "decoder_input_data = doc_vecs[:, :-1]\n",
    "decoder_target_data = doc_vecs[:, 1:]\n",
    "\n",
    "num_encoder_tokens = max(func_proc.id2token.keys()) + 1\n",
    "num_decoder_tokens = max(doc_proc.id2token.keys()) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, GRU, Embedding, Lambda, BatchNormalization\n",
    "\n",
    "word_emb_dim=800\n",
    "hidden_state_dim=1000\n",
    "encoder_seq_len=encoder_seq_len\n",
    "num_encoder_tokens=num_encoder_tokens\n",
    "num_decoder_tokens=num_decoder_tokens\n",
    "\n",
    "\n",
    "\n",
    "encoder_inputs = Input(shape=(encoder_seq_len,), name='Encoder-Input')\n",
    "# Word embeding for encoder (ex: Issue Titles, Code)\n",
    "x = Embedding(num_encoder_tokens, word_emb_dim, name='Body-Word-Embedding',\n",
    "              mask_zero=False)(encoder_inputs)\n",
    "x = BatchNormalization(name='Encoder-Batchnorm-1')(x)\n",
    "\n",
    "# We do not need the `encoder_output` just the hidden state.\n",
    "_, state_h = GRU(hidden_state_dim, return_state=True,\n",
    "                 name='Encoder-Last-GRU', dropout=.5)(x)\n",
    "\n",
    "# Encapsulate the encoder as a separate entity so we can just\n",
    "#  encode without decoding if we want to.\n",
    "encoder_model = Model(inputs=encoder_inputs, outputs=state_h,\n",
    "                      name='Encoder-Model')\n",
    "\n",
    "seq2seq_encoder_out = encoder_model(encoder_inputs)\n",
    "\n",
    "#### Decoder Model ####\n",
    "# for teacher forcing\n",
    "decoder_inputs = Input(shape=(None,), name='Decoder-Input')\n",
    "\n",
    "# Word Embedding For Decoder (ex: Issue Titles, Docstrings)\n",
    "dec_emb = Embedding(num_decoder_tokens, word_emb_dim,\n",
    "                    name='Decoder-Word-Embedding',\n",
    "                    mask_zero=False)(decoder_inputs)\n",
    "dec_bn = BatchNormalization(name='Decoder-Batchnorm-1')(dec_emb)\n",
    "\n",
    "# Set up the decoder, using `decoder_state_input` as initial state.\n",
    "decoder_gru = GRU(hidden_state_dim, return_state=True,\n",
    "                  return_sequences=True, name='Decoder-GRU', dropout=.5)\n",
    "decoder_gru_output, _ = decoder_gru(dec_bn,\n",
    "                                    initial_state=seq2seq_encoder_out)\n",
    "x = BatchNormalization(name='Decoder-Batchnorm-2')(decoder_gru_output)\n",
    "\n",
    "# Dense layer for prediction\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax',\n",
    "                      name='Final-Output-Dense')\n",
    "decoder_outputs = decoder_dense(x)\n",
    "\n",
    "#### Seq2Seq Model ####\n",
    "seq2seq_model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "import numpy as np\n",
    "\n",
    "seq2seq_model.compile(optimizer=optimizers.Nadam(lr=0.00005),\n",
    "                      loss='sparse_categorical_crossentropy')\n",
    "\n",
    "batch_size = 1100\n",
    "epochs = 16\n",
    "history = seq2seq_model.fit([encoder_input_data, decoder_input_data],\n",
    "                            np.expand_dims(decoder_target_data, -1),\n",
    "                            batch_size=batch_size,\n",
    "                            epochs=epochs,\n",
    "                            validation_split=0.12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_decoder_model(model):\n",
    "    # the latent dimension is the dmeinsion of the hidden state passed from the encoder to the decoder.\n",
    "    latent_dim = model.get_layer('Encoder-Model').output_shape[-1]\n",
    "\n",
    "    # Reconstruct the input into the decoder\n",
    "    decoder_inputs = model.get_layer('Decoder-Input').input\n",
    "    dec_emb = model.get_layer('Decoder-Word-Embedding')(decoder_inputs)\n",
    "    dec_bn = model.get_layer('Decoder-Batchnorm-1')(dec_emb)\n",
    "\n",
    "    # Instead of setting the intial state from the encoder and forgetting about it, during inference\n",
    "    # we are not doing teacher forcing, so we will have to have a feedback loop from predictions back into\n",
    "    # the GRU, thus we define this input layer for the state so we can add this capability\n",
    "    gru_inference_state_input = Input(shape=(latent_dim,), name='hidden_state_input')\n",
    "\n",
    "    # we need to reuse the weights that is why we are getting this\n",
    "    # If you inspect the decoder GRU that we created for training, it will take as input\n",
    "    # 2 tensors -> (1) is the embedding layer output for the teacher forcing\n",
    "    #                  (which will now be the last step's prediction, and will be _start_ on the first time step)\n",
    "    #              (2) is the state, which we will initialize with the encoder on the first time step, but then\n",
    "    #                   grab the state after the first prediction and feed that back in again.\n",
    "    gru_out, gru_state_out = model.get_layer('Decoder-GRU')([dec_bn, gru_inference_state_input])\n",
    "\n",
    "    # Reconstruct dense layers\n",
    "    dec_bn2 = model.get_layer('Decoder-Batchnorm-2')(gru_out)\n",
    "    dense_out = model.get_layer('Final-Output-Dense')(dec_bn2)\n",
    "    decoder_model = Model([decoder_inputs, gru_inference_state_input],\n",
    "                          [dense_out, gru_state_out])\n",
    "    return decoder_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model = seq2seq_model.get_layer('Encoder-Model')\n",
    "decoder_model = extract_decoder_model(seq2seq_model)\n",
    "raw_input_text = train_func[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = doc_proc.padding_maxlen\n",
    "\n",
    "raw_tokenized = func_proc.transform([raw_input_text])\n",
    "encoding = encoder_model.predict(raw_tokenized)\n",
    "# we want to save the encoder's embedding before its updated by decoder\n",
    "#   because we can use that as an embedding for other tasks.\n",
    "original_encoding = encoding\n",
    "state_value = np.array(doc_proc.token2id['_start_']).reshape(1, 1)\n",
    "\n",
    "decoded_sentence = []\n",
    "stop_condition = False\n",
    "while not stop_condition:\n",
    "    preds, st = decoder_model.predict([state_value, encoding])\n",
    "\n",
    "    # We are going to ignore indices 0 (padding) and indices 1 (unknown)\n",
    "    # Argmax will return the integer index corresponding to the\n",
    "    #  prediction + 2 b/c we chopped off first two\n",
    "    pred_idx = np.argmax(preds[:, :, 2:]) + 2\n",
    "\n",
    "    # retrieve word from index prediction\n",
    "    pred_word_str = doc_proc.id2token[pred_idx]\n",
    "\n",
    "    if pred_word_str == '_end_' or len(decoded_sentence) >= max_len:\n",
    "        stop_condition = True\n",
    "        break\n",
    "    decoded_sentence.append(pred_word_str)\n",
    "\n",
    "    # update the decoder for the next word\n",
    "    encoding = st\n",
    "    state_value = np.array(pred_idx).reshape(1, 1)\n",
    "\n",
    "print(' '.join(decoded_sentence))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
