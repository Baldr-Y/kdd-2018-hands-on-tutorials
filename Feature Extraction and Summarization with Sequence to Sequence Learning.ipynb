{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "czSR_WOeHqea"
   },
   "source": [
    "# KDD 2018 Hands-On Tutorial  https://kddseq2seq.com/\n",
    "\n",
    "Feature Extraction and Summarization With Sequence-to-Sequence Learning\n",
    "\n",
    "\n",
    "### Pre-requisites\n",
    "\n",
    "The target audience of this tutorial are moderately skilled users who have some familiarity with neural networks and are comfortable writing code.  These blog posts are good background for this tutorial:\n",
    "\n",
    "- [How To Create Data Products That Are Magical Using Sequence-to-Sequence Models](https://towardsdatascience.com/how-to-create-data-products-that-are-magical-using-sequence-to-sequence-models-703f86a231f8)\n",
    "\n",
    "- [How To Create Natural Language Semantic Search For Arbitrary Objects With Deep Learning](https://towardsdatascience.com/semantic-code-search-3cd6d244a39c)\n",
    "\n",
    "### Google Colab Notebooks\n",
    "\n",
    "This tutorial can be run in Google Colab notebooks, which provides a free gpu-enabled Jupyter Notebook on the cloud.  **You can open this notebook in Colab  by following [this link](https://colab.research.google.com/github/hohsiangwu/kdd-2018-hands-on-tutorials/blob/master/Feature%20Extraction%20and%20Summarization%20with%20Sequence%20to%20Sequence%20Learning.ipynb).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q-gKT37Didb7"
   },
   "source": [
    "# Setup Notebook\n",
    "\n",
    "Install [ktext](https://github.com/hamelsmu/ktext) and [annoy](https://github.com/spotify/annoy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E7l80u-0fyHK"
   },
   "outputs": [],
   "source": [
    "!pip install -q ktext\n",
    "!pip install -q annoy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "W-KjInFk0v8l",
    "outputId": "e8547e15-c6ae-4e07-9fb1-c56023312183"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from urllib.request import urlopen\n",
    "\n",
    "from annoy import AnnoyIndex\n",
    "from keras import optimizers\n",
    "from keras.layers import Input, Dense, LSTM, GRU, Embedding, Lambda, BatchNormalization\n",
    "from keras.models import Model\n",
    "from keras import optimizers\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from ktext.preprocess import processor\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iZzcs-PDPZqn"
   },
   "source": [
    "# Data sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "McfeOLlh0v8x"
   },
   "source": [
    "## [CoNaLa](https://conala-corpus.github.io/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "colab_type": "code",
    "id": "JAAOUd-k0v8x",
    "outputId": "7c3ef9e0-82de-4409-c5d4-44d7167dd39a"
   },
   "outputs": [],
   "source": [
    "!wget http://www.phontron.com/download/conala-corpus-v1.1.zip\n",
    "!unzip -o conala-corpus-v1.1.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hFzeLKxg0v81"
   },
   "outputs": [],
   "source": [
    "with open('conala-corpus/conala-mined.jsonl', 'r') as f:\n",
    "    lines = [json.loads(line) for line in f.readlines()]\n",
    "source_docs = [line['snippet'] for line in lines]\n",
    "target_docs = [line['intent'] for line in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QsXzuRH50v83"
   },
   "outputs": [],
   "source": [
    "with open('conala-corpus/conala-train.json', 'r') as f:\n",
    "    lines = json.load(f)\n",
    "train_source_docs = [line['snippet'] for line in lines]\n",
    "train_target_docs = [line['intent'] for line in lines]\n",
    "test_docs = [line['rewritten_intent'] for line in lines if line['rewritten_intent']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-t0x8A5w0v87"
   },
   "outputs": [],
   "source": [
    "with open('conala-corpus/conala-test.json', 'r') as f:\n",
    "    lines = json.load(f)\n",
    "test_source_docs = [line['snippet'] for line in lines]\n",
    "test_target_docs = [line['intent'] for line in lines]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EhmB0qpr3RD5"
   },
   "source": [
    "## Other Data Sources (For Later Use)\n",
    "\n",
    "The below datasets are alternate sources of data for this same exercise.  We will not be reviewing these data as part of this tutorial.  However, we encourage you to inspect these data for additional practice and to get more intuition regarding these techniques.  Practicing with these other datasets will  give you confidence regarding the general application of the techniques we are teaching in this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oQLV9P8h0v8q"
   },
   "source": [
    "### [English to French](http://www.manythings.org/anki/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1uK2D4090v8r"
   },
   "outputs": [],
   "source": [
    "# !wget http://www.manythings.org/anki/fra-eng.zip\n",
    "# !unzip -o fra-eng.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8UMp9B4M0v8u"
   },
   "outputs": [],
   "source": [
    "# with open('fra.txt', 'r') as f:\n",
    "#     lines = f.readlines()\n",
    "# target_docs, source_docs = zip(*[line.strip().split('\\t') for line in lines])\n",
    "# target_docs = list(set(target_docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bw33N_AcPZqp"
   },
   "source": [
    "### GitHub issues data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GSR2uek3PZqq"
   },
   "outputs": [],
   "source": [
    "# issues = pd.read_csv('https://storage.googleapis.com/kubeflow-examples/github-issue-summarization-data/github-issues.zip')\n",
    "# source_docs = list(issues.body)\n",
    "# target_docs = list(issues.issue_title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AP58__OgPZqt"
   },
   "source": [
    "### Python functions data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "izpU8EXGPZqu"
   },
   "outputs": [],
   "source": [
    "# f = urlopen('https://storage.googleapis.com/kubeflow-examples/code_search/data/train.function')\n",
    "# source_docs = [line.decode('utf-8') for line in f.readlines()]\n",
    "# f = urlopen('https://storage.googleapis.com/kubeflow-examples/code_search/data/train.docstring')\n",
    "# target_docs = [line.decode('utf-8') for line in f.readlines()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8JykNMln0v9I"
   },
   "source": [
    "## Use subset of the data\n",
    "\n",
    "We will use only of the training set in the interest of brevity.  However, we can use the full dataset in a subsequent pass if desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Hj1tDqDFPZqx"
   },
   "outputs": [],
   "source": [
    "source_docs = source_docs[:100000]\n",
    "target_docs = target_docs[:100000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lxYjchOjPZqz"
   },
   "source": [
    "# Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "COkelzEXPZq0"
   },
   "source": [
    "## Preprocessing\n",
    "Tokenize, generate vocabulary, apply padding and vectorize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hT8NcmEU4C4A"
   },
   "source": [
    "Lets inspect the raw text of the target docs.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "Ww4AHHUX4CPL",
    "outputId": "bb91d64e-a95b-4d29-d18c-7cae715b5fa1"
   },
   "outputs": [],
   "source": [
    "target_docs[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LTxQMjaa4WsL"
   },
   "source": [
    "In order to pre-process this text for deep learning, we need to convert this text into integer values.  In order to do this, we will use the `ktext` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "uHOMxrONPZq3",
    "outputId": "75152c95-15c7-44e1-9fa1-fd953dbc0db4"
   },
   "outputs": [],
   "source": [
    "proc = processor(hueristic_pct_padding=.7, keep_n=20000)\n",
    "vecs = proc.fit_transform(target_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0jld4HT0Dp_r"
   },
   "source": [
    "Below is an example where tokens are mapped to integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "-ogmGKqUC-Jd",
    "outputId": "6a8cc3dd-a432-47a7-9573-e5ee9d880144"
   },
   "outputs": [],
   "source": [
    "print('original list: ', target_docs[0].lower().split())\n",
    "print('tokenized list: ', [proc.token2id[x] for x in target_docs[0].lower().split()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ldna19QcEXYp"
   },
   "source": [
    "We can see the most common words here, by calling the `token_count_pandas()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 669
    },
    "colab_type": "code",
    "id": "dNOluhQpD8Fq",
    "outputId": "1c63804c-1f30-4889-be52-634c28e70aa2"
   },
   "outputs": [],
   "source": [
    "proc.token_count_pandas().head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ab3JU2L5Ey9_"
   },
   "source": [
    "Furthermore, the documents in our corpus have different lengths. By setting `hueristic_pct_padding=.7`, `ktext` will truncate and pad all sequences to the 70th percentile length. However, it can be useful to sanity check a histogram of lengths. We inspect the `document_length_stats` property below which displays a histogram of document lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 266
    },
    "colab_type": "code",
    "id": "ZIH4R1HREiTO",
    "outputId": "a7c2c1f2-b562-4c25-dc99-9a45a3b34784"
   },
   "outputs": [],
   "source": [
    "proc.document_length_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SCNn24hRDxeF"
   },
   "source": [
    "It is useful to keep track of the maximum length and the unique number of tokens in the corpus for later purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "wnjhEa-NPZq6",
    "outputId": "a95f37c6-608f-4c29-e6c4-4c814d7c8eaa"
   },
   "outputs": [],
   "source": [
    "vocab_size = max(proc.id2token.keys()) + 1\n",
    "max_length = proc.padding_maxlen\n",
    "\n",
    "print('vocab size: ', vocab_size)\n",
    "print('max length allowed for documents: ', max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "srQ2EApW0v9X"
   },
   "source": [
    "## Language model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V31lnrq70v9T"
   },
   "source": [
    "Prepare training data for language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Ioq7Sd9EPZq8",
    "outputId": "10a21d72-b9ef-49e1-f84a-0ca5aa91b33f"
   },
   "outputs": [],
   "source": [
    "sequences = []\n",
    "for arr in tqdm(vecs):\n",
    "    non_zero = (arr != 0).argmax()\n",
    "    for i in range(non_zero, len(arr)):\n",
    "        sequences.append(arr[:i+1])\n",
    "sequences = pad_sequences(sequences, maxlen=max_length, padding='pre')\n",
    "sequences = np.array(sequences)\n",
    "X, y = sequences[:,:-1], sequences[:,-1]\n",
    "# y = to_categorical(y, num_classes=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "id": "WKW7TCl6PZq_",
    "outputId": "c459d163-845a-483f-9e33-7faac193c784"
   },
   "outputs": [],
   "source": [
    "i = Input(shape=(max_length-1,))\n",
    "x = Embedding(vocab_size, 256, input_length=max_length-1)(i)\n",
    "x = LSTM(256, return_sequences=True)(x)\n",
    "last_timestep = Lambda(lambda x: x[:, -1, :])(x)\n",
    "last_timestep = Dense(vocab_size, activation='softmax')(last_timestep)\n",
    "model = Model(i, last_timestep)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jnVeYIZV0v9a"
   },
   "source": [
    "## Training\n",
    "\n",
    "Now that we have created our architecture, we can train our model.  \n",
    "\n",
    "**This step takes approximately 20 minutes.  This is a good time to take a bathroom break!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 326
    },
    "colab_type": "code",
    "id": "Psve_dWVPZrB",
    "outputId": "2c404204-3821-499b-ef9a-94b74c6dda83"
   },
   "outputs": [],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "history = model.fit(X, y, epochs=10, batch_size=50, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cFH1DmGHPZrE"
   },
   "source": [
    "## Generate sequence\n",
    "\n",
    "The goal of a language model is to predict the next word in a sequence. To sanity check the language model, we will see what kind of sentence is generated when we start with a a seed word of 'is'. We are looking to see if the sentence generated appears to be sampled from the distribution of the data.\n",
    "\n",
    "In other words does the sentence generated look like it was written by the same author(s) pertaining to the same domain as the training corpus?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2R-2I_HRPZrE"
   },
   "outputs": [],
   "source": [
    "def generate_seq(model, proc, n_words, seed_text):\n",
    "    in_text = seed_text\n",
    "    for _ in range(n_words):\n",
    "        vec = proc.transform([in_text])[:,1:]\n",
    "        index = np.argmax(model.predict(vec, verbose=0), axis=1)[0]\n",
    "        out_word = ''\n",
    "        if index == 1:\n",
    "            out_word = '_unk_'\n",
    "        else:\n",
    "            out_word = proc.id2token[index]\n",
    "        in_text += ' ' + out_word\n",
    "    return in_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MCDhy30KqF2j"
   },
   "source": [
    "See what sentence is generated from language model, seeded witht he word `is`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "8wpHSxIOPZrH",
    "outputId": "76a3e606-c5ae-4394-c76a-cd1e7abdcdb7"
   },
   "outputs": [],
   "source": [
    "generate_seq(model, proc, max_length, 'is')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zzNZqTqVPZrJ"
   },
   "source": [
    "## Generate sentence embeddings\n",
    "\n",
    "One of the goals of training the language model is learning reprsentations of sentences in our corpus. For example, we can extract values from intermediate layers of this language model, and use those as sentence embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y-Rig9cePZrM"
   },
   "outputs": [],
   "source": [
    "embedding_model = Model(inputs=model.inputs, outputs=model.layers[-3].output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fW1UeLKWqkMp"
   },
   "source": [
    "The below code extracts the hidden states from the encoder when given an input. There is one hidden state for each word in the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "id": "X-FFznLQPZrO",
    "outputId": "8b3e342d-76f5-4c00-bc39-b6b84e1d624d"
   },
   "outputs": [],
   "source": [
    "input_sequence = test_docs[random.randint(0, len(test_docs))]\n",
    "print('input sequence: ', input_sequence, '\\n\\nhidden states:\\n')\n",
    "vec = proc.transform([input_sequence])[:,1:]\n",
    "embedding_model.predict(vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PBksOOsJrTkk"
   },
   "source": [
    "Let's extract the hidden states for all the sentences in our training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "haQjC19y0v9l"
   },
   "outputs": [],
   "source": [
    "vecs = proc.transform(test_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rLeLELte0v9n"
   },
   "outputs": [],
   "source": [
    "hidden_states = embedding_model.predict(vecs[:, 1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aF90GHpSraaK"
   },
   "source": [
    "To create a sentence embedding, we need to summarize the hidden states (there is one for each term). A simple approach is to use aggregate stastics like the mean, max, or the sum of all the hidden states. There are other approaches that are outside the scope of this notebook, but that we will discuss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SCf0kjpr0v9p"
   },
   "outputs": [],
   "source": [
    "mean_vecs = np.mean(hidden_states, axis=1)\n",
    "max_vecs = np.max(hidden_states, axis=1)\n",
    "sum_vecs = np.sum(hidden_states, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "giC48Kt00v9s"
   },
   "source": [
    "## Application - Nearest Neighbor Search\n",
    "\n",
    "Now that we have a way to represent each sentence as a vector, we can use this representation on many kinds of downstream tasks. One such task is finding a similar sentence to any given sentence.\n",
    "\n",
    "### Build vector indices\n",
    "\n",
    "We will first place all the vectorized sentences in a special data structure that allows for fast nearest neighbor lookups. We will use [annoy](https://github.com/spotify/annoy) for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "afBInH4C0v9s",
    "outputId": "beb89caf-ebf5-4c00-fea9-20cc156313da"
   },
   "outputs": [],
   "source": [
    "dimension = hidden_states.shape[-1]\n",
    "index = AnnoyIndex(dimension)\n",
    "for i, v in enumerate(sum_vecs):\n",
    "    index.add_item(i, v)\n",
    "index.build(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pp2KAWNj0v9u"
   },
   "source": [
    "### Search nearest neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "colab_type": "code",
    "id": "tr_tKEDx0v9w",
    "outputId": "d05cc768-fc1b-4729-cbea-78d22346f105"
   },
   "outputs": [],
   "source": [
    "input_sequence = test_docs[random.randint(0, len(test_docs))]\n",
    "print('Query: ', input_sequence)\n",
    "\n",
    "vec = proc.transform([input_sequence])[:,1:]\n",
    "vec = np.sum(embedding_model.predict(vec), axis=1)\n",
    "ids, _ = index.get_nns_by_vector(vec.T, 10, include_distances=True)\n",
    "\n",
    "print('Search Results:')\n",
    "[test_docs[i] for i in ids]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pnPW20o9PZrR"
   },
   "source": [
    "# Sequence to Sequence Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VpxbvU6OCDje"
   },
   "source": [
    "A [sequence to sequence model](https://towardsdatascience.com/how-to-create-data-products-that-are-magical-using-sequence-to-sequence-models-703f86a231f8) allows you to take an input sequence (source), and predict an output sequence (target).  These sequences can be anything, however we will focus on natural language for this tutorial. Sequence-to-sequence models have been used with great success in summarizing texts as well as generating translations from one language to another. For this tutorial, we will demonstrate a very creative task: given a snippet of code, we will train a model that generates a description of that code!\n",
    "\n",
    "Let's take a look at the data we want to use. The `source` is the snippet of code and the `target` is the description of that code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "ESQyR0l2CH01",
    "outputId": "4fc6d9ac-d8df-4f86-93ab-dffa436d2d4a"
   },
   "outputs": [],
   "source": [
    "print('source (code input): ', source_docs[2])\n",
    "print('target (description output): ', target_docs[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BQsnI6720v9z"
   },
   "source": [
    "## Preprocessing\n",
    "\n",
    "Similar to previous excercises, we must pre-process the raw strings into a format that can be utilized by our model. One such format is to map each word in our corpus to a unique integer value, which we will refer to as a vocabulary. If the source and target are from the same distribution, (which they are not in this example) the vocabulary can be shared.\n",
    "\n",
    "\n",
    "Concretely, we will tokenize, generate vocabulary, apply padding and vectorize. These steps are as follows:\n",
    "\n",
    "**1. Tokenize:** Process of parsing strings into discrete words or tokens.\n",
    "\n",
    "**2. Generate Vocabulary:** Assign each token to a unique integer, rare-occuring tokens may be assigned to the same integer.\n",
    "\n",
    "**3. Padding:** We standardize the sequence length of each example to be the same by truncating and padding each example to the same lentgh.\n",
    "\n",
    "The `ktext` package helps us accomplish these steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "colab_type": "code",
    "id": "s_nE-SeKPZrS",
    "outputId": "2cf383fc-8126-4e42-a7ac-e206c8053cdb"
   },
   "outputs": [],
   "source": [
    "source_proc = processor(hueristic_pct_padding=.7, keep_n=20000)\n",
    "source_vecs = source_proc.fit_transform(source_docs)\n",
    "\n",
    "target_proc = processor(append_indicators=True, hueristic_pct_padding=.7, keep_n=14000, padding ='post')\n",
    "target_vecs = target_proc.fit_transform(target_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qkfDO83HE8Xc"
   },
   "source": [
    "We will use teacher forcing for the decoder of the sequence to sequence model, so we will offset the target sequence by one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3b0wFryNPZrU"
   },
   "outputs": [],
   "source": [
    "encoder_input_data = source_vecs\n",
    "encoder_seq_len = encoder_input_data.shape[1]\n",
    "\n",
    "decoder_input_data = target_vecs[:, :-1]\n",
    "decoder_target_data = target_vecs[:, 1:]\n",
    "\n",
    "num_encoder_tokens = max(source_proc.id2token.keys()) + 1\n",
    "num_decoder_tokens = max(target_proc.id2token.keys()) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FCkBg1_1PZrZ"
   },
   "source": [
    "## Encoder model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wvDkJOqoFHhf"
   },
   "source": [
    "The role of the encoder is to extract features and generate a representation of the input sequence, which in this case is a snippet of code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nThkgdknPZra"
   },
   "outputs": [],
   "source": [
    "word_emb_dim=512\n",
    "hidden_state_dim=1024\n",
    "encoder_seq_len=encoder_seq_len\n",
    "num_encoder_tokens=num_encoder_tokens\n",
    "num_decoder_tokens=num_decoder_tokens\n",
    "\n",
    "encoder_inputs = Input(shape=(encoder_seq_len,), name='Encoder-Input')\n",
    "x = Embedding(num_encoder_tokens, word_emb_dim, name='Body-Word-Embedding', mask_zero=False)(encoder_inputs)\n",
    "x = BatchNormalization(name='Encoder-Batchnorm-1')(x)\n",
    "_, state_h = GRU(hidden_state_dim, return_state=True, name='Encoder-Last-GRU', dropout=.5)(x)\n",
    "encoder_model = Model(inputs=encoder_inputs, outputs=state_h, name='Encoder-Model')\n",
    "seq2seq_encoder_out = encoder_model(encoder_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8PnsgReVPZre"
   },
   "source": [
    "## Decoder model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t9IbLqTCFQr3"
   },
   "source": [
    "The role of the decoder is to generate a description of the code conditioned on the features extracted by the encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bu-KgmfjPZre"
   },
   "outputs": [],
   "source": [
    "decoder_inputs = Input(shape=(None,), name='Decoder-Input')\n",
    "dec_emb = Embedding(num_decoder_tokens, word_emb_dim, name='Decoder-Word-Embedding', mask_zero=False)(decoder_inputs)\n",
    "dec_bn = BatchNormalization(name='Decoder-Batchnorm-1')(dec_emb)\n",
    "decoder_gru = GRU(hidden_state_dim, return_state=True, return_sequences=True, name='Decoder-GRU', dropout=.5)\n",
    "decoder_gru_output, _ = decoder_gru(dec_bn, initial_state=seq2seq_encoder_out)\n",
    "x = BatchNormalization(name='Decoder-Batchnorm-2')(decoder_gru_output)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax', name='Final-Output-Dense')\n",
    "decoder_outputs = decoder_dense(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WfH-WvGwPZrg"
   },
   "source": [
    "## Sequence to sequence model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kW3bg3XnFYqx"
   },
   "source": [
    "We can connect the encoder and decoder together to create the sequence to sequence model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xsNgKYlKPZri"
   },
   "outputs": [],
   "source": [
    "seq2seq_model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WT8maZlD0v-A"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JxHP00h6FgL0"
   },
   "source": [
    "The below hyperparameters were found through some trial and error.\n",
    "\n",
    "**This should take approximately ~ 35 minutes to train.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 578
    },
    "colab_type": "code",
    "id": "HgUemXCbPZrk",
    "outputId": "b43c882b-28fc-431c-903c-76f7d475870a"
   },
   "outputs": [],
   "source": [
    "batch_size = 1024\n",
    "epochs = 16\n",
    "\n",
    "seq2seq_model.compile(optimizer=optimizers.Nadam(lr=0.00005), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "history = seq2seq_model.fit([encoder_input_data, decoder_input_data],\n",
    "                            np.expand_dims(decoder_target_data, -1),\n",
    "                            batch_size=batch_size,\n",
    "                            epochs=epochs,\n",
    "                            validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1lbdGPHE0v-B"
   },
   "source": [
    "## Extract encoder and decoder models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xcl66yGRF11e"
   },
   "source": [
    "For inference, we will not have teacher forcing for the decoder. Therefore, we must re-assemble our model such we can feed one prediction at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UAqk3fIwPZrn"
   },
   "outputs": [],
   "source": [
    "def extract_decoder_model(model):\n",
    "    latent_dim = model.get_layer('Encoder-Model').output_shape[-1]\n",
    "    decoder_inputs = model.get_layer('Decoder-Input').input\n",
    "    dec_emb = model.get_layer('Decoder-Word-Embedding')(decoder_inputs)\n",
    "    dec_bn = model.get_layer('Decoder-Batchnorm-1')(dec_emb)\n",
    "    gru_inference_state_input = Input(shape=(latent_dim,), name='hidden_state_input')\n",
    "    gru_out, gru_state_out = model.get_layer('Decoder-GRU')([dec_bn, gru_inference_state_input])\n",
    "    dec_bn2 = model.get_layer('Decoder-Batchnorm-2')(gru_out)\n",
    "    dense_out = model.get_layer('Final-Output-Dense')(dec_bn2)\n",
    "    decoder_model = Model([decoder_inputs, gru_inference_state_input], [dense_out, gru_state_out])\n",
    "    return decoder_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PU8JuOvfGKta"
   },
   "source": [
    "One side effect of training a sequence-to-sequence model in this way is that the encoder can be re-used as a general purpose feature extractor. We extract the encoder below for this purpose in a later exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "colab_type": "code",
    "id": "Xe0_KakGPZrp",
    "outputId": "b800b3f1-557a-4863-92d9-9c0d330d93c8"
   },
   "outputs": [],
   "source": [
    "encoder_model = seq2seq_model.get_layer('Encoder-Model')\n",
    "for layer in encoder_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "decoder_model = extract_decoder_model(seq2seq_model)\n",
    "decoder_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZFdwR6UR0v-L"
   },
   "source": [
    "## Predict code descriptions using the trained sequence-to-sequence model\n",
    "\n",
    "You will see that the predicted descriptions are not perfect, but seem to be picking up on correlations between common code token sequences and natural language descriptions of that code.\n",
    "\n",
    "Feel free to run the below block of code as many times as you want. A new random sample from the test set will be drawn each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "JvK1eN9fPZrr",
    "outputId": "382926e0-1bd4-43f3-bf7d-0aaf065a9296"
   },
   "outputs": [],
   "source": [
    "i = random.randint(0, len(test_source_docs))\n",
    "\n",
    "max_len = target_proc.padding_maxlen\n",
    "raw_input_text = test_source_docs[i]\n",
    "\n",
    "raw_tokenized = source_proc.transform([raw_input_text])\n",
    "encoding = encoder_model.predict(raw_tokenized)\n",
    "original_encoding = encoding\n",
    "state_value = np.array(target_proc.token2id['_start_']).reshape(1, 1)\n",
    "\n",
    "decoded_sentence = []\n",
    "stop_condition = False\n",
    "while not stop_condition:\n",
    "    preds, st = decoder_model.predict([state_value, encoding])\n",
    "    pred_idx = np.argmax(preds[:, :, 2:]) + 2\n",
    "    pred_word_str = target_proc.id2token[pred_idx]\n",
    "\n",
    "    if pred_word_str == '_end_' or len(decoded_sentence) >= max_len:\n",
    "        stop_condition = True\n",
    "        break\n",
    "    decoded_sentence.append(pred_word_str)\n",
    "\n",
    "    # update the decoder for the next word\n",
    "    encoding = st\n",
    "    state_value = np.array(pred_idx).reshape(1, 1)\n",
    "\n",
    "print('sample code from test set:\\n------------------------\\n', raw_input_text)\n",
    "print('\\nground truth:\\n------------------------\\n', test_target_docs[i])\n",
    "print('\\npredicted description:\\n------------------------')\n",
    "print(' '.join(decoded_sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tf2bITb3Ny7H"
   },
   "source": [
    "## Generate Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qYPM3Tqf0v-P"
   },
   "outputs": [],
   "source": [
    "train_source_emb = encoder_model.predict(source_proc.transform(train_source_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "geb3SOyk0v-Q"
   },
   "outputs": [],
   "source": [
    "vecs = proc.transform(train_target_docs)\n",
    "hidden_states = embedding_model.predict(vecs[:, 1:])\n",
    "mean_vecs = np.mean(hidden_states, axis=1)\n",
    "max_vecs = np.max(hidden_states, axis=1)\n",
    "sum_vecs = np.sum(hidden_states, axis=1)\n",
    "train_target_emb = sum_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "e_f5ptIL0v-R",
    "outputId": "14af1f07-113d-4248-b593-ea6601072504"
   },
   "outputs": [],
   "source": [
    "print('source embedding shape on training set: ', train_source_emb.shape)\n",
    "print('target embedding shape on training set: ', train_target_emb.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8xOKEWFj0v-S"
   },
   "source": [
    "# Construct a Joint Vector Space\n",
    "\n",
    "Right now we have a way of representing:\n",
    "- a blob of code as a vector using the encoder of the sequence-to-sequence model, and \n",
    "- the code descriptions as a vector using the language model.\n",
    "\n",
    "However, these two vector spaces are not related to eachother. It can be useful to project the vectors for code and descriptions into the same space so that we can search code with natural language. There are many ways of accomplishing this task, however we will demonstrate a technique inspired from [this paper](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/41473.pdf), where we use regression to \"pull\" these vectors into the same space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "-noqpPJY0v-T",
    "outputId": "a41d437d-1238-4b71-ac3c-9b00b28260fe"
   },
   "outputs": [],
   "source": [
    "inp = Input(shape=(train_source_emb.shape[1],))\n",
    "x = Dense(train_target_emb.shape[1], use_bias=False)(inp)\n",
    "# x = BatchNormalization()(x)\n",
    "# x = Dense(512)(x)\n",
    "modal_model = Model([inp], x)\n",
    "modal_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "colab_type": "code",
    "id": "7zALzqWO0v-U",
    "outputId": "8c67a690-b38d-4942-ce31-d7676e587099"
   },
   "outputs": [],
   "source": [
    "modal_model.compile(optimizer=optimizers.Nadam(lr=0.002), loss='cosine_proximity', metrics=['accuracy'])\n",
    "\n",
    "batch_size = 1024\n",
    "epochs = 10\n",
    "history = modal_model.fit([train_source_emb], train_target_emb,\n",
    "                          batch_size=batch_size, epochs=epochs, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Re8nxdPs0v-Y"
   },
   "source": [
    "## Application - Semantic Search\n",
    "\n",
    "### Use test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZX0AIo_B0v-Z"
   },
   "outputs": [],
   "source": [
    "test_source_emb = encoder_model.predict(source_proc.transform(test_source_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2BaHfbsl0v-a"
   },
   "outputs": [],
   "source": [
    "vecs = proc.transform(test_target_docs)\n",
    "hidden_states = embedding_model.predict(vecs[:, 1:])\n",
    "mean_vecs = np.mean(hidden_states, axis=1)\n",
    "max_vecs = np.max(hidden_states, axis=1)\n",
    "sum_vecs = np.sum(hidden_states, axis=1)\n",
    "test_target_emb = sum_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "QgKykGVh0v-b",
    "outputId": "6fbebb62-fd19-4652-8e97-f4a1dc48a34a"
   },
   "outputs": [],
   "source": [
    "print(test_source_emb.shape)\n",
    "print(test_target_emb.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eg7JX1Vu0v-d"
   },
   "source": [
    "### Build vector indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "D7HKpc3j0v-d",
    "outputId": "05a144e8-8f73-4fa5-e871-ba700aa73928"
   },
   "outputs": [],
   "source": [
    "dimension = hidden_states.shape[-1]\n",
    "index = AnnoyIndex(dimension)\n",
    "for i, v in enumerate(test_target_emb):\n",
    "    index.add_item(i, v)\n",
    "index.build(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C-fpIqvQ0v-f"
   },
   "source": [
    "### Search nearest neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "zPlMXs500v-g",
    "outputId": "594732f1-d8d9-4444-8222-9c7f9b8c12a4"
   },
   "outputs": [],
   "source": [
    "i = random.randint(0, len(test_source_docs))\n",
    "input_sequence = test_source_docs[i]\n",
    "print(input_sequence)\n",
    "\n",
    "vec = np.expand_dims(test_source_emb[i], 0)\n",
    "out_vec = modal_model.predict(vec)\n",
    "ids, _ = index.get_nns_by_vector(out_vec.T, 10, include_distances=True)\n",
    "[test_target_docs[i] for i in ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U-u1WX4EQa83"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Feature Extraction and Summarization with Sequence to Sequence Learning.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
