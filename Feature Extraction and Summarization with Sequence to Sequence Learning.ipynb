{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Feature Extraction and Summarization with Sequence to Sequence Learning.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/hamelsmu/kdd-2018-hands-on-tutorials/blob/master/Feature%20Extraction%20and%20Summarization%20with%20Sequence%20to%20Sequence%20Learning.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "Q-gKT37Didb7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Setup Notebook\n",
        "\n",
        "Install [ktext](https://github.com/hamelsmu/ktext)"
      ]
    },
    {
      "metadata": {
        "id": "E7l80u-0fyHK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "bd962878-59b3-4ba1-9fdd-852e66a0fc21"
      },
      "cell_type": "code",
      "source": [
        "! pip install ktext > install_logs.txt"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[31mthinc 6.10.2 has requirement cytoolz<0.9,>=0.8, but you'll have cytoolz 0.9.0.1 which is incompatible.\u001b[0m\r\n",
            "\u001b[31mthinc 6.10.2 has requirement msgpack-numpy==0.4.1, but you'll have msgpack-numpy 0.4.3.1 which is incompatible.\u001b[0m\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "iZzcs-PDPZqn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Data sets"
      ]
    },
    {
      "metadata": {
        "id": "bw33N_AcPZqp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## GitHub issues data"
      ]
    },
    {
      "metadata": {
        "id": "GSR2uek3PZqq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "679c8aab-f736-45b5-bcc9-a5985ba90ddb"
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "issues = pd.read_csv('https://storage.googleapis.com/kubeflow-examples/github-issue-summarization-data/github-issues.zip')\n",
        "source_docs = list(issues.body)\n",
        "target_docs = list(issues.issue_title)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AP58__OgPZqt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Python functions data"
      ]
    },
    {
      "metadata": {
        "id": "izpU8EXGPZqu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "749c6f86-cbbd-4e94-fc60-da357ecd44a3"
      },
      "cell_type": "code",
      "source": [
        "from urllib.request import urlopen\n",
        "f = urlopen('https://storage.googleapis.com/kubeflow-examples/code_search/data/train.function')\n",
        "source_docs = [x.decode('utf-8') for x in f.readlines()]\n",
        "f = urlopen('https://storage.googleapis.com/kubeflow-examples/code_search/data/train.docstring')\n",
        "target_docs = [x.decode('utf-8') for x in f.readlines()]"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Hj1tDqDFPZqx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "02cc63fd-caca-4d72-8187-ce1cae460349"
      },
      "cell_type": "code",
      "source": [
        "source_docs = source_docs[:1000]\n",
        "target_docs = target_docs[:1000]"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lxYjchOjPZqz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 1: Language Model"
      ]
    },
    {
      "metadata": {
        "id": "COkelzEXPZq0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Input Data"
      ]
    },
    {
      "metadata": {
        "id": "h3inmC0oPZq1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8dc826b1-ff89-485f-81a8-86849f04feeb"
      },
      "cell_type": "code",
      "source": [
        "from keras import optimizers\n",
        "from keras.layers import Input, Dense, LSTM, GRU, Embedding, Lambda, BatchNormalization\n",
        "from keras.models import Model\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from ktext.preprocess import processor\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "uHOMxrONPZq3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "e01e8ae3-0960-453e-aa9c-87737eeef701"
      },
      "cell_type": "code",
      "source": [
        "proc = processor(hueristic_pct_padding=.7, keep_n=20000)\n",
        "vecs = proc.fit_transform(source_docs)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:....tokenizing data\n",
            "WARNING:root:Setting maximum document length to 60 based upon hueristic of 0.7 percentile.\n",
            " See full histogram by insepecting the `document_length_stats` attribute.\n",
            "WARNING:root:(1/2) done. 0 sec\n",
            "WARNING:root:....building corpus\n",
            "WARNING:root:(2/2) done. 0 sec\n",
            "WARNING:root:Finished parsing 1,000 documents.\n",
            "WARNING:root:...fit is finished, beginning transform\n",
            "WARNING:root:...padding data\n",
            "WARNING:root:done. 0 sec\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "wnjhEa-NPZq6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "c53c079e-fe55-4ab2-9e25-0baefc19cbb3"
      },
      "cell_type": "code",
      "source": [
        "vocab_size = max(proc.id2token.keys()) + 1\n",
        "max_length = proc.padding_maxlen"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ioq7Sd9EPZq8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2c1e11a5-b12d-424c-ece2-1cf97c745615"
      },
      "cell_type": "code",
      "source": [
        "sequences = []\n",
        "for arr in tqdm(vecs):\n",
        "    non_zero = (arr != 0).argmax()\n",
        "    for i in range(non_zero, len(arr)):\n",
        "        sequences.append(arr[:i+1])\n",
        "sequences = pad_sequences(sequences, maxlen=max_length, padding='pre')\n",
        "sequences = np.array(sequences)\n",
        "X, y = sequences[:,:-1], sequences[:,-1]\n",
        "y = to_categorical(y, num_classes=vocab_size)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [00:00<00:00, 24795.04it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "WKW7TCl6PZq_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "2279454b-3470-470f-8451-5f98d6deacf2"
      },
      "cell_type": "code",
      "source": [
        "i = Input(shape=(max_length-1,))\n",
        "o = Embedding(vocab_size, 128, input_length=max_length-1)(i)\n",
        "o = LSTM(50, return_sequences=True)(o)\n",
        "last_timestep = Lambda(lambda x: x[:, -1, :])(o)\n",
        "last_timestep = Dense(vocab_size, activation='softmax')(last_timestep)\n",
        "model = Model(i, last_timestep)\n",
        "model.summary()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 59)                0         \n",
            "_________________________________________________________________\n",
            "embedding_1 (Embedding)      (None, 59, 128)           390016    \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 59, 50)            35800     \n",
            "_________________________________________________________________\n",
            "lambda_1 (Lambda)            (None, 50)                0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 3047)              155397    \n",
            "=================================================================\n",
            "Total params: 581,213\n",
            "Trainable params: 581,213\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Psve_dWVPZrB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 717
        },
        "outputId": "605c91e7-1138-4caf-ec03-069a4320e971"
      },
      "cell_type": "code",
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "history = model.fit(X, y, epochs=20, batch_size=2048)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "37455/37455 [==============================] - 8s 225us/step - loss: 7.9886 - acc: 0.0187\n",
            "Epoch 2/20\n",
            "37455/37455 [==============================] - 5s 138us/step - loss: 7.3803 - acc: 0.0188\n",
            "Epoch 3/20\n",
            "37455/37455 [==============================] - 5s 143us/step - loss: 6.6055 - acc: 0.0475\n",
            "Epoch 4/20\n",
            "37455/37455 [==============================] - 5s 143us/step - loss: 6.3713 - acc: 0.0475\n",
            "Epoch 5/20\n",
            "37455/37455 [==============================] - 5s 141us/step - loss: 6.3423 - acc: 0.0475\n",
            "Epoch 6/20\n",
            "37455/37455 [==============================] - 5s 142us/step - loss: 6.3363 - acc: 0.0475\n",
            "Epoch 7/20\n",
            "37455/37455 [==============================] - 5s 142us/step - loss: 6.3330 - acc: 0.0475\n",
            "Epoch 8/20\n",
            "37455/37455 [==============================] - 5s 142us/step - loss: 6.3290 - acc: 0.0475\n",
            "Epoch 9/20\n",
            "37455/37455 [==============================] - 5s 138us/step - loss: 6.3227 - acc: 0.0475\n",
            "Epoch 10/20\n",
            "18432/37455 [=============>................] - ETA: 2s - loss: 6.3057 - acc: 0.0487"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "37455/37455 [==============================] - 5s 137us/step - loss: 6.3131 - acc: 0.0475\n",
            "Epoch 11/20\n",
            "37455/37455 [==============================] - 5s 138us/step - loss: 6.2935 - acc: 0.0475\n",
            "Epoch 12/20\n",
            "37455/37455 [==============================] - 5s 131us/step - loss: 6.2671 - acc: 0.0475\n",
            "Epoch 13/20\n",
            "37455/37455 [==============================] - 5s 131us/step - loss: 6.2446 - acc: 0.0475\n",
            "Epoch 14/20\n",
            "37455/37455 [==============================] - 5s 129us/step - loss: 6.2278 - acc: 0.0475\n",
            "Epoch 15/20\n",
            "37455/37455 [==============================] - 5s 128us/step - loss: 6.2139 - acc: 0.0475\n",
            "Epoch 16/20\n",
            "37455/37455 [==============================] - 5s 128us/step - loss: 6.2019 - acc: 0.0556\n",
            "Epoch 17/20\n",
            "37455/37455 [==============================] - 5s 127us/step - loss: 6.1895 - acc: 0.0643\n",
            "Epoch 18/20\n",
            "37455/37455 [==============================] - 5s 128us/step - loss: 6.1778 - acc: 0.0631\n",
            "Epoch 19/20\n",
            "20480/37455 [===============>..............] - ETA: 2s - loss: 6.1736 - acc: 0.0618"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "37455/37455 [==============================] - 5s 128us/step - loss: 6.1654 - acc: 0.0630\n",
            "Epoch 20/20\n",
            "37455/37455 [==============================] - 5s 135us/step - loss: 6.1536 - acc: 0.0632\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "cFH1DmGHPZrE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Generate sequences"
      ]
    },
    {
      "metadata": {
        "id": "2R-2I_HRPZrE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "969f0b6b-bd3d-4fe5-f2aa-267261b5129c"
      },
      "cell_type": "code",
      "source": [
        "def generate_seq(model, proc, max_length, seed_text, n_words):\n",
        "    in_text = seed_text\n",
        "    for _ in range(n_words):\n",
        "        vec = proc.transform([in_text])[:,1:]\n",
        "        index = np.argmax(model.predict(vec, verbose=0), axis=1)[0]\n",
        "        out_word = ''\n",
        "        if index == 1:\n",
        "            out_word = '_unk_'\n",
        "        else:\n",
        "            out_word = proc.id2token[index]\n",
        "        in_text += ' ' + out_word\n",
        "    return in_text"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8wpHSxIOPZrH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "38e052f0-cdfe-461c-ca0d-9556f6e9a62b"
      },
      "cell_type": "code",
      "source": [
        "generate_seq(model, proc, max_length, 'there', 10)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'there self self self self self self self self self self'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "metadata": {
        "id": "zzNZqTqVPZrJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Generate embeddings"
      ]
    },
    {
      "metadata": {
        "id": "4mlJZechPZrK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "2bb1907d-29b6-472e-d264-dba6b3cc2db6"
      },
      "cell_type": "code",
      "source": [
        "input_sequence = 'def machine learning'\n",
        "vec = proc.transform([input_sequence])[:,1:]"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Y-Rig9cePZrM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "77a9944a-b827-490f-a9e3-53c773380ff1"
      },
      "cell_type": "code",
      "source": [
        "embedding_model = Model(inputs=model.inputs, outputs=model.layers[-3].output)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "metadata": {
        "id": "X-FFznLQPZrO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "5241595b-98c8-433e-9cd2-a22ad6954c92"
      },
      "cell_type": "code",
      "source": [
        "embedding_model.predict(vec)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[ 0.11232524, -0.19589685, -0.08292498, ...,  0.11231813,\n",
              "         -0.04093925,  0.2250973 ],\n",
              "        [ 0.33013782, -0.50921947, -0.33424243, ...,  0.3724169 ,\n",
              "         -0.22250906,  0.5329745 ],\n",
              "        [ 0.710988  , -0.83309007, -0.7124429 , ...,  0.7527665 ,\n",
              "         -0.6078541 ,  0.82568073],\n",
              "        ...,\n",
              "        [ 1.        , -1.        , -0.7387216 , ...,  1.        ,\n",
              "         -1.        ,  0.36398363],\n",
              "        [ 0.8879844 , -0.92908955, -0.699541  , ...,  0.9367341 ,\n",
              "         -0.97065234,  0.451724  ],\n",
              "        [ 0.9398087 , -0.9845134 , -0.74498785, ...,  0.975736  ,\n",
              "         -1.        ,  0.48387393]]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "metadata": {
        "id": "pnPW20o9PZrR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 2: Sequence to Sequence Model"
      ]
    },
    {
      "metadata": {
        "id": "s_nE-SeKPZrS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "25b0b32e-2fdc-4b0b-ab9d-c7003520f00d"
      },
      "cell_type": "code",
      "source": [
        "from ktext.preprocess import processor\n",
        "source_proc = processor(hueristic_pct_padding=.7, keep_n=20000)\n",
        "source_vecs = source_proc.fit_transform(source_docs)\n",
        "\n",
        "target_proc = processor(append_indicators=True, hueristic_pct_padding=.7, keep_n=14000, padding ='post')\n",
        "target_vecs = target_proc.fit_transform(target_docs)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:....tokenizing data\n",
            "WARNING:root:Setting maximum document length to 60 based upon hueristic of 0.7 percentile.\n",
            " See full histogram by insepecting the `document_length_stats` attribute.\n",
            "WARNING:root:(1/2) done. 0 sec\n",
            "WARNING:root:....building corpus\n",
            "WARNING:root:(2/2) done. 0 sec\n",
            "WARNING:root:Finished parsing 1,000 documents.\n",
            "WARNING:root:...fit is finished, beginning transform\n",
            "WARNING:root:...padding data\n",
            "WARNING:root:done. 0 sec\n",
            "WARNING:root:....tokenizing data\n",
            "WARNING:root:Setting maximum document length to 20 based upon hueristic of 0.7 percentile.\n",
            " See full histogram by insepecting the `document_length_stats` attribute.\n",
            "WARNING:root:(1/2) done. 0 sec\n",
            "WARNING:root:....building corpus\n",
            "WARNING:root:(2/2) done. 0 sec\n",
            "WARNING:root:Finished parsing 1,000 documents.\n",
            "WARNING:root:...fit is finished, beginning transform\n",
            "WARNING:root:...padding data\n",
            "WARNING:root:done. 0 sec\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "3b0wFryNPZrU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "e45a8a9d-032d-4434-ba46-69cee4d72ca4"
      },
      "cell_type": "code",
      "source": [
        "encoder_input_data = source_vecs\n",
        "encoder_seq_len = encoder_input_data.shape[1]\n",
        "\n",
        "decoder_input_data = target_vecs[:, :-1]\n",
        "decoder_target_data = target_vecs[:, 1:]\n",
        "\n",
        "num_encoder_tokens = max(source_proc.id2token.keys()) + 1\n",
        "num_decoder_tokens = max(target_proc.id2token.keys()) + 1"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FCkBg1_1PZrZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Encoder Model"
      ]
    },
    {
      "metadata": {
        "id": "nThkgdknPZra",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "d9d99e54-4d45-4872-c7a2-c933de5e14ca"
      },
      "cell_type": "code",
      "source": [
        "word_emb_dim=800\n",
        "hidden_state_dim=1000\n",
        "encoder_seq_len=encoder_seq_len\n",
        "num_encoder_tokens=num_encoder_tokens\n",
        "num_decoder_tokens=num_decoder_tokens\n",
        "\n",
        "encoder_inputs = Input(shape=(encoder_seq_len,), name='Encoder-Input')\n",
        "x = Embedding(num_encoder_tokens, word_emb_dim, name='Body-Word-Embedding', mask_zero=False)(encoder_inputs)\n",
        "x = BatchNormalization(name='Encoder-Batchnorm-1')(x)\n",
        "_, state_h = GRU(hidden_state_dim, return_state=True, name='Encoder-Last-GRU', dropout=.5)(x)\n",
        "encoder_model = Model(inputs=encoder_inputs, outputs=state_h, name='Encoder-Model')\n",
        "seq2seq_encoder_out = encoder_model(encoder_inputs)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8PnsgReVPZre",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Decoder Model"
      ]
    },
    {
      "metadata": {
        "id": "bu-KgmfjPZre",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "ca0de062-3f55-47f2-c2f1-ca39722c00d7"
      },
      "cell_type": "code",
      "source": [
        "decoder_inputs = Input(shape=(None,), name='Decoder-Input')\n",
        "dec_emb = Embedding(num_decoder_tokens, word_emb_dim, name='Decoder-Word-Embedding', mask_zero=False)(decoder_inputs)\n",
        "dec_bn = BatchNormalization(name='Decoder-Batchnorm-1')(dec_emb)\n",
        "decoder_gru = GRU(hidden_state_dim, return_state=True, return_sequences=True, name='Decoder-GRU', dropout=.5)\n",
        "decoder_gru_output, _ = decoder_gru(dec_bn, initial_state=seq2seq_encoder_out)\n",
        "x = BatchNormalization(name='Decoder-Batchnorm-2')(decoder_gru_output)\n",
        "decoder_dense = Dense(num_decoder_tokens, activation='softmax', name='Final-Output-Dense')\n",
        "decoder_outputs = decoder_dense(x)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WfH-WvGwPZrg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## End to end"
      ]
    },
    {
      "metadata": {
        "id": "xsNgKYlKPZri",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "3647fd51-06f5-4da3-ea81-37f160b0f22f"
      },
      "cell_type": "code",
      "source": [
        "seq2seq_model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HgUemXCbPZrk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 578
        },
        "outputId": "001c30a4-891b-4652-a584-252aa96abb46"
      },
      "cell_type": "code",
      "source": [
        "batch_size = 1100\n",
        "epochs = 16\n",
        "\n",
        "seq2seq_model.compile(optimizer=optimizers.Nadam(lr=0.00005), loss='sparse_categorical_crossentropy')\n",
        "history = seq2seq_model.fit([encoder_input_data, decoder_input_data],\n",
        "                            np.expand_dims(decoder_target_data, -1),\n",
        "                            batch_size=batch_size,\n",
        "                            epochs=epochs,\n",
        "                            validation_split=0.12)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 880 samples, validate on 120 samples\n",
            "Epoch 1/16\n",
            "880/880 [==============================] - 5s 6ms/step - loss: 8.2666 - val_loss: 7.5461\n",
            "Epoch 2/16\n",
            "880/880 [==============================] - 2s 2ms/step - loss: 7.8155 - val_loss: 7.1328\n",
            "Epoch 3/16\n",
            "880/880 [==============================] - 2s 2ms/step - loss: 7.5178 - val_loss: 6.7729\n",
            "Epoch 4/16\n",
            "880/880 [==============================] - 2s 2ms/step - loss: 7.2603 - val_loss: 6.4247\n",
            "Epoch 5/16\n",
            "880/880 [==============================] - 2s 2ms/step - loss: 6.9972 - val_loss: 6.0886\n",
            "Epoch 6/16\n",
            "880/880 [==============================] - 2s 2ms/step - loss: 6.7542 - val_loss: 5.7908\n",
            "Epoch 7/16\n",
            "880/880 [==============================] - 2s 2ms/step - loss: 6.5194 - val_loss: 5.5685\n",
            "Epoch 8/16\n",
            "880/880 [==============================] - 2s 2ms/step - loss: 6.3010 - val_loss: 5.4265\n",
            "Epoch 9/16\n",
            "880/880 [==============================] - 2s 2ms/step - loss: 6.1242 - val_loss: 5.3343\n",
            "Epoch 10/16\n",
            "880/880 [==============================] - 2s 2ms/step - loss: 5.9849 - val_loss: 5.2648\n",
            "Epoch 11/16\n",
            "880/880 [==============================] - 2s 2ms/step - loss: 5.8851 - val_loss: 5.2049\n",
            "Epoch 12/16\n",
            "880/880 [==============================] - 2s 2ms/step - loss: 5.7891 - val_loss: 5.1492\n",
            "Epoch 13/16\n",
            "880/880 [==============================] - 2s 2ms/step - loss: 5.7150 - val_loss: 5.0958\n",
            "Epoch 14/16\n",
            "880/880 [==============================] - 2s 2ms/step - loss: 5.6415 - val_loss: 5.0439\n",
            "Epoch 15/16\n",
            "880/880 [==============================] - 2s 2ms/step - loss: 5.5705 - val_loss: 4.9926\n",
            "Epoch 16/16\n",
            "880/880 [==============================] - 2s 2ms/step - loss: 5.5232 - val_loss: 4.9417\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "UAqk3fIwPZrn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "d8e93394-1741-4f29-a219-6b34479617af"
      },
      "cell_type": "code",
      "source": [
        "def extract_decoder_model(model):\n",
        "    latent_dim = model.get_layer('Encoder-Model').output_shape[-1]\n",
        "    decoder_inputs = model.get_layer('Decoder-Input').input\n",
        "    dec_emb = model.get_layer('Decoder-Word-Embedding')(decoder_inputs)\n",
        "    dec_bn = model.get_layer('Decoder-Batchnorm-1')(dec_emb)\n",
        "    gru_inference_state_input = Input(shape=(latent_dim,), name='hidden_state_input')\n",
        "    gru_out, gru_state_out = model.get_layer('Decoder-GRU')([dec_bn, gru_inference_state_input])\n",
        "    dec_bn2 = model.get_layer('Decoder-Batchnorm-2')(gru_out)\n",
        "    dense_out = model.get_layer('Final-Output-Dense')(dec_bn2)\n",
        "    decoder_model = Model([decoder_inputs, gru_inference_state_input], [dense_out, gru_state_out])\n",
        "    return decoder_model"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Xe0_KakGPZrp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "eeaad795-a14b-4cac-a534-2c8b177004ef"
      },
      "cell_type": "code",
      "source": [
        "encoder_model = seq2seq_model.get_layer('Encoder-Model')\n",
        "decoder_model = extract_decoder_model(seq2seq_model)\n",
        "decoder_model.summary()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "Decoder-Input (InputLayer)      (None, None)         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "Decoder-Word-Embedding (Embeddi (None, None, 800)    1719200     Decoder-Input[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "Decoder-Batchnorm-1 (BatchNorma (None, None, 800)    3200        Decoder-Word-Embedding[1][0]     \n",
            "__________________________________________________________________________________________________\n",
            "hidden_state_input (InputLayer) (None, 1000)         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "Decoder-GRU (GRU)               [(None, None, 1000), 5403000     Decoder-Batchnorm-1[1][0]        \n",
            "                                                                 hidden_state_input[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "Decoder-Batchnorm-2 (BatchNorma (None, None, 1000)   4000        Decoder-GRU[1][0]                \n",
            "__________________________________________________________________________________________________\n",
            "Final-Output-Dense (Dense)      (None, None, 2149)   2151149     Decoder-Batchnorm-2[1][0]        \n",
            "==================================================================================================\n",
            "Total params: 9,280,549\n",
            "Trainable params: 9,276,949\n",
            "Non-trainable params: 3,600\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "JvK1eN9fPZrr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "916bf747-d2a6-4784-ea7f-e74d203d8dc6"
      },
      "cell_type": "code",
      "source": [
        "max_len = target_proc.padding_maxlen\n",
        "raw_input_text = source_docs[0]\n",
        "\n",
        "raw_tokenized = source_proc.transform([raw_input_text])\n",
        "encoding = encoder_model.predict(raw_tokenized)\n",
        "original_encoding = encoding\n",
        "state_value = np.array(target_proc.token2id['_start_']).reshape(1, 1)\n",
        "\n",
        "decoded_sentence = []\n",
        "stop_condition = False\n",
        "while not stop_condition:\n",
        "    preds, st = decoder_model.predict([state_value, encoding])\n",
        "    pred_idx = np.argmax(preds[:, :, 2:]) + 2\n",
        "    pred_word_str = target_proc.id2token[pred_idx]\n",
        "\n",
        "    if pred_word_str == '_end_' or len(decoded_sentence) >= max_len:\n",
        "        stop_condition = True\n",
        "        break\n",
        "    decoded_sentence.append(pred_word_str)\n",
        "\n",
        "    # update the decoder for the next word\n",
        "    encoding = st\n",
        "    state_value = np.array(pred_idx).reshape(1, 1)\n",
        "\n",
        "' '.join(decoded_sentence)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'a icon the cancelled parameter client infra channelfilter dependency numbers happened used made sentences client traversal into emit internal builds'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "metadata": {
        "id": "1xgPRKJqPZrv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}